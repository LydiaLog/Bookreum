# import os
# import pandas as pd
# import numpy as np
# import json
# import re
# import sys
# import requests
# import html
# from sqlalchemy import create_engine, text
# from sentence_transformers import SentenceTransformer
# import faiss
# from transformers import pipeline  # âœ… ê°ì • ë¶„ì„ìš©

# # ì„¤ì •
# ALADIN_API_URL = 'https://www.aladin.co.kr/ttb/api/ItemSearch.aspx'
# DB_URL = "mysql+pymysql://root:7h4saf0324!@localhost:3306/bookreum?charset=utf8mb4"
# engine = create_engine(DB_URL)
# session = engine.connect()

# # ëª¨ë¸ ë¡œë”©
# model = SentenceTransformer('all-MiniLM-L6-v2')
# emotion_pipeline = pipeline("sentiment-analysis", model="nlptown/bert-base-multilingual-uncased-sentiment")

# # ì±… ê²€ìƒ‰ í•¨ìˆ˜
# def fetch_books_from_aladin(keyword, max_results=50):
#     try:
#         params = {
#             'ttbkey': 'ttbwonj011527001',
#             'Query': keyword,
#             'QueryType': 'Title',
#             'MaxResults': max_results,
#             'Output': 'js',
#             'Cover': 'Big'
#         }
#         response = requests.get(ALADIN_API_URL, params=params)
#         response_text = html.unescape(response.text.strip())
#         response_text = re.sub(r'\\(?![nrt"\\/])', '', response_text)

#         match = re.search(r'\{.*\}', response_text, re.DOTALL)
#         if not match:
#             return pd.DataFrame()

#         data = json.loads(match.group(0))
#         books = pd.DataFrame([{ 
#             'title': item.get('title', ''),
#             'description': item.get('description', ''),
#             'author': item.get('author', ''),
#             'cover_image_url': item.get('cover', '')
#         } for item in data.get('item', [])])
#         return books
#     except Exception as e:
#         print(json.dumps({"error": f"API ì˜¤ë¥˜: {str(e)}"}))
#         return pd.DataFrame()

# # ì¶”ì²œ ì´ìœ  ìƒì„±
# def generate_reason(title, keyword):
#     return f"'{title}'ëŠ” '{keyword}'ì™€ ìœ ì‚¬í•œ ì£¼ì œë¥¼ ë‹¤ë£¨ëŠ” ì±…ì…ë‹ˆë‹¤."

# # ëª…ë ¹ì–´ ì¸ì ë°›ê¸°
# if len(sys.argv) < 2:
#     print(json.dumps({"error": "í‚¤ì›Œë“œê°€ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}))
#     sys.exit(1)

# keyword = sys.argv[1]
# related_keywords = [keyword]

# # ì „ì²´ ë„ì„œ ìˆ˜ì§‘
# books = pd.DataFrame()
# for kw in related_keywords:
#     books = pd.concat([books, fetch_books_from_aladin(kw, max_results=10)])
# books.drop_duplicates(subset=['title'], inplace=True)

# # ìœ ì‚¬ë„ ë¶„ì„ìš© ì„ë² ë”©
# descriptions = books["description"].fillna("").astype(str).tolist()
# if not descriptions:
#     print(json.dumps({"error": "ì¶”ì²œí•  ì±… ì„¤ëª…ì´ ì—†ìŠµë‹ˆë‹¤."}))
#     sys.exit(1)

# embeddings = model.encode(descriptions, normalize_embeddings=True)
# d = embeddings.shape[1]
# index = faiss.IndexFlatIP(d)
# faiss.normalize_L2(embeddings)
# index.add(embeddings.astype("float32"))

# query_embedding = model.encode([keyword], normalize_embeddings=True)
# faiss.normalize_L2(query_embedding)
# D, I = index.search(query_embedding.astype("float32"), 8)

# # ì¶”ì²œ ê²°ê³¼ ìƒì„±
# recommended_books = []
# for idx in I[0]:
#     row = books.iloc[idx]
#     if keyword in row['title']:
#         continue  # ì œëª©ì— í‚¤ì›Œë“œ í¬í•¨ëœ ì±… ì œì™¸
#     reason = generate_reason(row['title'], keyword)
#     recommended_books.append({
#         "title": row['title'],
#         "author": row['author'],
#         "cover_image_url": row['cover_image_url'],
#         "recommend_reason": reason
#     })

# # ì¤‘ë³µ ì œê±° ë° ìµœëŒ€ 8ê°œ
# unique_books = []
# seen_titles = set()
# for book in recommended_books:
#     if book['title'] not in seen_titles:
#         unique_books.append(book)
#         seen_titles.add(book['title'])
#         if len(unique_books) >= 8:
#             break

# # ğŸ” ë¶€ì¡±í•  ê²½ìš° ê°ì • ê¸°ë°˜ ì¶”ì²œ ì¶”ê°€
# if len(unique_books) < 8:
#     try:
#         sentiment_result = emotion_pipeline(keyword)[0]
#         emotion = sentiment_result['label']
#         additional_books_df = fetch_books_from_aladin(emotion, max_results=10)

#         for _, row in additional_books_df.iterrows():
#             if row['title'] in seen_titles or keyword in row['title']:
#                 continue
#             unique_books.append({
#                 "title": row['title'],
#                 "author": row['author'],
#                 "cover_image_url": row['cover_image_url'],
#                 "recommend_reason": f"'{row['title']}'ëŠ” '{keyword}'ì™€ ë¹„ìŠ·í•œ ê°ì •ì¸ '{emotion}' ë¶„ìœ„ê¸°ë¥¼ ë‹´ê³  ìˆìŠµë‹ˆë‹¤."
#             })
#             seen_titles.add(row['title'])
#             if len(unique_books) >= 8:
#                 break
#     except Exception as e:
#         print(json.dumps({"error": f"ê°ì • ê¸°ë°˜ ì¶”ì²œ ì˜¤ë¥˜: {str(e)}"}))

# # DB ì €ì¥
# try:
#     for book in unique_books:
#         existing = session.execute(
#             text("SELECT id FROM RecommendedBook WHERE title = :title AND author = :author"),
#             {"title": book['title'], "author": book['author']}
#         ).fetchone()
#         if not existing:
#             session.execute(text("""
#                 INSERT INTO RecommendedBook (title, author, cover_image_url, recommend_reason, aladin_rating)
#                 VALUES (:title, :author, :cover_image_url, :recommend_reason, NULL)
#             """), book)
#     session.commit()
# except Exception as e:
#     session.rollback()
#     print(json.dumps({"error": f"DB ì €ì¥ ì˜¤ë¥˜: {str(e)}"}))
# finally:
#     session.close()

# # ì¶œë ¥
# recommended_titles = [book['title'] for book in unique_books]
# print(json.dumps({"recommended_titles": recommended_titles}, ensure_ascii=False))
import os
import pandas as pd
from sentence_transformers import SentenceTransformer
import numpy as np
import json
import re
import sys
import requests
from sqlalchemy import create_engine, text
from transformers import pipeline

# âœ… ì•Œë¼ë”˜ API URL ì„¤ì •
ALADIN_API_URL = 'https://www.aladin.co.kr/ttb/api/ItemSearch.aspx'

# âœ… MySQL ì—°ê²° ì„¤ì • (ë³¸ì¸ í™˜ê²½ì— ë§ê²Œ ìˆ˜ì •)
DB_URL = "mysql+pymysql://root:7h4saf0324!@localhost:3306/bookreum?charset=utf8mb4"
engine = create_engine(DB_URL)
session = engine.connect()

# âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë”©
model = SentenceTransformer('all-MiniLM-L6-v2')

# âœ… API ìš”ì²­ í•¨ìˆ˜ (ì±… ê²€ìƒ‰)
def fetch_books_from_aladin(keyword, max_results=50):
    params = {
        'ttbkey': 'ttbwonj011527001',
        'Query': keyword,
        'QueryType': 'Title',
        'MaxResults': max_results,
        'Output': 'js',
        'Cover': 'Big'
    }
    response = requests.get(ALADIN_API_URL, params=params)
    response_text = response.text.strip()
    # print("ğŸ” API ì‘ë‹µ ì›ë³¸:", response_text[:500], "...")

    try:
        cleaned_text = re.search(r'(\{.*\})', response_text, re.DOTALL)
        if cleaned_text:
            json_text = cleaned_text.group(1).replace('\\', '\\\\')
            data = json.loads(json_text)
            books = pd.DataFrame([{ 
                'title': item.get('title', ''),
                'description': item.get('description', ''),
                'author': item.get('author', ''),
                'cover_image_url': item.get('cover', '')
            } for item in data.get('item', [])])
            # print(f"âœ… APIì—ì„œ ê²€ìƒ‰ëœ ì±… ìˆ˜: {len(books)}")
            return books
        else:
            # print("âŒ API ì‘ë‹µì´ JSON í˜•ì‹ì´ ì•„ë‹˜")
            return pd.DataFrame()
    except json.JSONDecodeError as e:
        # print(f"âŒ JSON íŒŒì‹± ì˜¤ë¥˜: {str(e)}")
        # print("ğŸ” API ì‘ë‹µ ì „ì²´:", response_text)
        return pd.DataFrame()

# âœ… ì‚¬ìš©ì ì…ë ¥
if len(sys.argv) < 2:
    print(json.dumps({"error": "ê¸°ì¤€ ì±… ì œëª©ì´ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}))
    sys.exit(1)

keyword = sys.argv[1]

# âœ… ê¸°ì¤€ ì±… ì„¤ëª… í™•ë³´
base_book_df = fetch_books_from_aladin(keyword, max_results=1)
if base_book_df.empty:
    print(json.dumps({"error": "ê¸°ì¤€ ì±…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}))
    sys.exit(1)

base_description = base_book_df.iloc[0]['description']

# âœ… ì¼ë°˜ í‚¤ì›Œë“œë¡œ ì±… ìˆ˜ì§‘
topics = ["ì†Œì„¤", "ë¬¸í•™", "ì¸ìƒ", "ê°ì •", "ê³ ì „", "ì² í•™", "ì‹¬ë¦¬", "ì‚¶"]
books = pd.DataFrame()
for kw in topics:
    books = pd.concat([books, fetch_books_from_aladin(kw, max_results=30)])

books.drop_duplicates(subset=['title'], inplace=True)
books = books[books['description'].notnull()]

# âœ… í•„í„°ë§: ì œì™¸ í‚¤ì›Œë“œ
exclude_keywords = [
    'ìŠ¤íƒ€ì¼', 'ìœ ì•„', 'ìœ ì•„ë„ì„œ', 'íšŒí™”', 'ìŠ¤í‹°ì»¤', 'ìºë¦­í„°', 'í‹°ë‹ˆí•‘', 'ì¡ì§€', 'ìˆ˜í—˜ì„œ', 
    'í•™ìŠµ', 'ì–´ë¦°ì´', 'ì–´ë¦°ì´ë„ì„œ', 'ì–´ë¦°ì´ì±…', 'ì–´ë¦°ì´ë„ê°', 'ì–´ë¦°ì´ë°±ê³¼ì‚¬ì „', 
    'ì–´ë¦°ì´ê·¸ë¦¼ì±…', 'ì–´ë¦°ì´ì†Œì„¤', 'ì–´ë¦°ì´ë™í™”', 'ì–´ë¦°ì´ë§Œí™”ì±…', 'ì–´ë¦°ì´ì „ì§‘', 
    'ì–´ë¦°ì´ë¬¸ê³ ', 'ì–´ë¦°ì´ë¬¸í•™', 'ì–´ë¦°ì´ê³¼í•™ë„ì„œ', 'êµì‚¬', 'ì„ìš©', 'ìœ ì¹˜ì›',
    'ì´ˆë“±', 'ì¤‘ë“±', 'ê³ ë“±', 'ëŒ€í•™', 'ëŒ€í•™ìƒ', 'ëŒ€í•™ì›ìƒ', 'ëŒ€í•™ì›', 'ì „ë¬¸ì„œì ',
    'ì „ë¬¸ê°€', 'ì „ë¬¸ì§', 'ì „ë¬¸ì„œì ', 'ì „ë¬¸ê°€ìš©', 'ì „ë¬¸ì§ì¢…', 'ì „ë¬¸ì§ì—…', 'í•™ë…„', 'ì‹œë¦¬ì¦ˆ', 'ì„¸íŠ¸',
    'ì „ì§‘', 'ë¬¸ê³ ', 'ë°±ê³¼ì‚¬ì „', 'ì‚¬ì „', 'êµê³¼ì„œ', 'ì°¸ê³ ì„œ', 'ë¬¸ì œì§‘', 'í•™ìŠµì§€',
    'ìƒ', 'í•˜', 'ë³„'
]
books = books[~books['title'].str.contains('|'.join(exclude_keywords), case=False, na=False)]

# âœ… ì„ë² ë”© ê¸°ë°˜ ìœ ì‚¬ë„ ê³„ì‚°
if books.empty or not base_description:
    print(json.dumps({"error": "ì¶”ì²œí•  ì±… ì„¤ëª…ì´ ì—†ìŠµë‹ˆë‹¤."}))
    sys.exit(1)

from sklearn.metrics.pairwise import cosine_similarity

descriptions = books['description'].tolist()
book_embeddings = model.encode(descriptions, normalize_embeddings=True)
query_embedding = model.encode([base_description], normalize_embeddings=True)

similarities = cosine_similarity(query_embedding, book_embeddings)[0]
top_indices = similarities.argsort()[::-1]

# âœ… ì¶”ì²œ ê²°ê³¼ êµ¬ì„±
recommended_books = []
seen_titles = set()
for idx in top_indices:
    row = books.iloc[idx]
    if keyword in row['title'] or row['title'] in seen_titles:
        continue
    reason = f"ì´ ì±…ì€ '{keyword}'ì™€ ë¹„ìŠ·í•œ ê°ì •ê³¼ ê²½í—˜ì„ ì „ë‹¬í•©ë‹ˆë‹¤."
    recommended_books.append({
        "title": row['title'],
        "author": row['author'],
        "cover_image_url": row['cover_image_url'],
        "recommend_reason": reason
    })
    seen_titles.add(row['title'])
    if len(recommended_books) >= 8:
        break

# âœ… DB ì €ì¥ ë¡œì§
try:
    for book in recommended_books:
        existing_book = session.execute(
            text("SELECT id FROM RecommendedBook WHERE title = :title AND author = :author"),
            {"title": book['title'], "author": book['author']}
        ).fetchone()
        if not existing_book:
            session.execute(
                text("""
                    INSERT INTO RecommendedBook (title, author, cover_image_url, recommend_reason, aladin_rating)
                    VALUES (:title, :author, :cover_image_url, :recommend_reason, NULL)
                """),
                book
            )
            # print(f"âœ… DB ì €ì¥ ì„±ê³µ: {book['title']}\n")
        else:
            # print(f"âš ï¸ DBì— ì´ë¯¸ ì¡´ì¬: {book['title']}\n")
            pass
    session.commit()
except Exception as e:
    session.rollback()
    print(json.dumps({"error": f"DB ì €ì¥ ì˜¤ë¥˜: {str(e)}"}))
finally:
    session.close()

# âœ… ìµœì¢… JSON ì¶œë ¥
recommended_titles = [book['title'] for book in recommended_books]

print(json.dumps({
    "recommended_books": recommended_books,
    "recommended_titles": recommended_titles
}, ensure_ascii=False))
